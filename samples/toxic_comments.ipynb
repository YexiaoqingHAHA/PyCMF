{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Toxic Commments Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) was a competition to classify comments into various categories of toxicity.\n",
    "\n",
    "\n",
    "In this tutorial, we will see how CMF can extract relevant topics to classification using the toxicity labels as supervision, and how that cannot be accomplished using other topic modeling techniques.\n",
    "\n",
    "Before running this code, you'll need to download the data from [here](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data) and put it in a directory of your choice (make sure the below DATA variable points to the directory where the data resides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA / \"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n",
       "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  \n",
       "5             0        0       0       0              0  \n",
       "6             1        1       0       1              0  \n",
       "7             0        0       0       0              0  \n",
       "8             0        0       0       0              0  \n",
       "9             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805  \n",
       "std         0.216627       0.093420  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now construct the text data to do topic modeling on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=3, max_df=0.9, stop_words=\"english\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 577 ms, sys: 3.56 ms, total: 580 ms\n",
      "Wall time: 580 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = vectorizer.fit_transform(train.head(10000).comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 9927)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label data to decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix, csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_columns = list(train.columns[2:]); label_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train.head(10000)[label_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 6)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COMPONENTS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis functions we'll use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word = np.array(vectorizer.get_feature_names())\n",
    "def print_topics(H, topn=10):\n",
    "    for i, topic in enumerate(H): \n",
    "        print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in idx_to_word[topic.argsort()[-topn:]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what topics we find using NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(N_COMPONENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.68 s, sys: 10 ms, total: 3.69 s\n",
      "Wall time: 3.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "W = nmf.fit_transform(X_train); H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: used,right,case,said,point,make,fact,way,say,does\n",
      "Topic 2: tildes,hello,questions,date,welcome,sign,using,editing,pages,help\n",
      "Topic 3: placed,speedily,guidelines,tag,add,remove,speedy,note,deleted,deletion\n",
      "Topic 4: message,added,comments,user,wp,comment,discussion,redirect,contribs,talk\n",
      "Topic 5: pov,history,subject,discussion,good,main,created,needs,section,article\n",
      "Topic 6: user,said,want,doesn,say,care,understand,need,really,don\n",
      "Topic 7: considered,editors,users,http,stop,policy,en,wiki,org,wikipedia\n",
      "Topic 8: bit,new,sounds,hey,feel,thing,really,things,looks,like\n",
      "Topic 9: let,editor,right,mean,ll,really,going,wanted,say,just\n",
      "Topic 10: say,wiki,dont,got,thing,didn,want,did,let,know\n",
      "Topic 11: vandalize,make,stop,pages,continue,edits,editing,blocked,did,edit\n",
      "Topic 12: post,read,message,delete,links,new,link,discussion,user,page\n",
      "Topic 13: life,way,really,wiki,person,going,group,world,want,people\n",
      "Topic 14: way,added,new,lot,ll,work,help,good,hi,thanks\n",
      "Topic 15: right,way,section,sure,needs,idea,better,good,really,think\n",
      "Topic 16: reverted,learn,welcome,encyclopedia,sandbox,thank,removed,look,use,want\n",
      "Topic 17: new,really,utc,got,going,long,good,ll,ve,time\n",
      "Topic 18: published,references,www,reference,com,http,reliable,information,source,sources\n",
      "Topic 19: contributions,proposed,issues,policy,deletion,delete,consensus,edits,wp,articles\n",
      "Topic 20: jpg,uploaded,link,copyright,fair,deleted,thank,used,image,use\n"
     ]
    }
   ],
   "source": [
    "print_topics(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of these topics seems related to toxicity..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter alpha determines the trade-off between how much to value the decomposition of one matrix over the other when computing the loss. It only affects the results when using the newton solver.\n",
    "\n",
    "We'll use a very heuristic method of computing alpha here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1643312620900889"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xnorm = np.sqrt(X_train.multiply(X_train).sum())\n",
    "ynorm = np.sqrt((Y_train * Y_train).sum())\n",
    "alpha = (ynorm / (xnorm + ynorm)) ** 0.75; alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMF with non-negative constraint on all matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmf_nn = pycmf.CMF(N_COMPONENTS,\n",
    "                   U_non_negative=True, V_non_negative=True, Z_non_negative=True,\n",
    "                   alpha=alpha, solver=\"mu\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 reached after 6.567 seconds, error: 234.839634\n",
      "Epoch 20 reached after 13.161 seconds, error: 234.113588\n",
      "Epoch 30 reached after 19.860 seconds, error: 234.033264\n",
      "Epoch 40 reached after 27.461 seconds, error: 233.994436\n",
      "Epoch 50 reached after 36.267 seconds, error: 233.970200\n",
      "CPU times: user 25.4 s, sys: 11.1 s, total: 36.5 s\n",
      "Wall time: 36.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "U, V, Z = cmf_nn.fit_transform(csr_matrix(X_train.T), Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 [0.000,0.000,0.000,0.000,0.000,0.000]: right,said,case,used,point,fact,way,make,say,does\n",
      "Topic 2 [0.000,0.000,0.000,0.002,0.000,0.000]: date,hello,ask,welcome,sign,questions,using,editing,pages,help\n",
      "Topic 3 [0.000,0.000,0.000,0.000,0.000,0.000]: information,free,articles,tag,add,criteria,note,speedy,deletion,deleted\n",
      "Topic 4 [0.000,0.000,0.000,0.000,0.000,0.000]: remove,message,comments,comment,wp,user,redirect,discussion,contribs,talk\n",
      "Topic 5 [0.000,0.000,0.000,0.000,0.000,0.000]: version,history,wp,created,pov,main,needs,discussion,section,article\n",
      "Topic 6 [6.421,0.970,4.473,0.233,4.192,0.721]: dick,life,suck,bitch,stop,ass,stupid,shit,fucking,fuck\n",
      "Topic 7 [0.090,0.000,0.000,0.000,0.000,0.000]: vandalism,users,editors,stop,wiki,en,policy,org,articles,wikipedia\n",
      "Topic 8 [0.017,0.000,0.000,0.000,0.000,0.000]: hey,bit,really,thing,new,sounds,look,things,looks,like\n",
      "Topic 9 [0.004,0.000,0.000,0.000,0.000,0.000]: hi,got,didn,mean,ll,going,let,say,wanted,just\n",
      "Topic 10 [0.001,0.000,0.000,0.000,0.000,0.000]: say,ll,care,understand,ve,doesn,let,really,know,don\n",
      "Topic 11 [0.015,0.000,0.000,0.000,0.000,0.000]: ip,block,make,vandalize,stop,pages,continue,editing,blocked,did\n",
      "Topic 12 [0.016,0.000,0.000,0.000,0.000,0.000]: created,delete,post,links,pages,discussion,read,new,user,page\n",
      "Topic 13 [0.036,0.000,0.000,0.000,0.000,0.000]: person,life,really,years,long,world,know,want,time,people\n",
      "Topic 14 [0.002,0.000,0.000,0.000,0.000,0.000]: got,going,articles,work,ll,faith,really,time,ve,good\n",
      "Topic 15 [0.000,0.000,0.000,0.013,0.000,0.004]: saying,question,right,things,section,idea,needs,way,better,think\n",
      "Topic 16 [0.000,0.000,0.000,0.000,0.000,0.000]: contributing,welcome,learn,encyclopedia,sandbox,removed,look,thank,want,use\n",
      "Topic 17 [0.000,0.000,0.000,0.000,0.000,0.000]: 12,21,2006,16,23,15,2004,20,2005,utc\n",
      "Topic 18 [0.000,0.000,0.000,0.000,0.000,0.000]: published,references,reference,www,com,http,information,reliable,source,sources\n",
      "Topic 19 [0.003,0.000,0.000,0.010,0.000,0.000]: removing,warring,articles,war,consensus,added,reverted,summary,edits,edit\n",
      "Topic 20 [0.000,0.000,0.000,0.000,0.000,0.000]: uploading,fair,pages,copyright,image,help,link,list,hi,thanks\n"
     ]
    }
   ],
   "source": [
    "cmf_nn.print_topic_terms(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results seem better(?). Now we see some topics that are actually related to toxicity. Let's see what happens when we allow negativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMF with non-negative constraint on only the topic matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmf = pycmf.CMF(N_COMPONENTS,\n",
    "               U_non_negative=True, V_non_negative=True, Z_non_negative=False,\n",
    "               x_link=\"linear\", y_link=\"logit\", alpha=alpha, l1_reg=2., l2_reg=5., max_iter=10,\n",
    "               solver=\"newton\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 reached after 233.713 seconds, error: 135.584596\n",
      "CPU times: user 3min 34s, sys: 19.9 s, total: 3min 53s\n",
      "Wall time: 3min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "U, V, Z = cmf.fit_transform(csr_matrix(X_train.T), Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 [-0.047,-0.323,-0.057,-0.469,-0.015,-0.257]: hate,little,hell,life,hey,know,shit,ass,fucking,fuck\n",
      "Topic 2 [0.139,-3.046,-2.502,-3.067,-2.442,-3.153]: say,wikipedia,like,articles,know,just,time,think,people,don\n",
      "Topic 3 [-1.022,-0.368,0.075,-0.461,-0.314,-0.382]: non,ask,material,read,reason,article,page,want,wikipedia,deleted\n",
      "Topic 4 [-5.596,-5.963,-4.806,-6.224,-5.267,-6.076]: don,confirm,accepted,companies,indicate,year,bands,company,tagging,club\n",
      "Topic 5 [-6.810,-4.408,-6.029,-3.981,-5.526,-4.095]: way,page,wikipedia,right,talk,user,use,section,know,article\n",
      "Topic 6 [-0.698,-0.271,-0.237,-0.501,-0.341,-0.307]: editors,way,talk,does,article,make,wikipedia,page,questions,like\n",
      "Topic 7 [-4.465,-3.540,-4.501,-3.125,-4.039,-3.068]: edit,really,page,article,just,think,like,people,know,don\n",
      "Topic 8 [0.516,0.147,0.988,-0.568,0.920,0.194]: gets,avoid,stupid,right,man,suck,nazi,bitch,faggot,fuck\n",
      "Topic 9 [2.863,-1.721,1.025,-1.698,0.904,-1.538]: contributing,create,takes,learn,tests,policy,creating,test,message,sandbox\n",
      "Topic 10 [-0.938,-3.301,-1.568,-4.202,-1.609,-3.610]: deleting,discussion,utc,vandalism,request,blocked,stop,edit,page,talk\n",
      "Topic 11 [-2.268,-1.443,-1.591,-1.337,-2.139,-1.483]: air,ago,saying,details,small,maybe,came,updated,talking,referred\n",
      "Topic 12 [3.440,-0.909,1.348,-1.103,2.061,-0.957]: suck,hell,dumb,idiot,stop,shit,ass,stupid,fucking,fuck\n",
      "Topic 13 [-0.554,-0.394,-0.740,-0.573,-0.644,-0.358]: case,information,page,article,sources,fact,discussion,used,good,think\n",
      "Topic 14 [0.976,-1.307,-0.466,-1.178,-0.199,-1.252]: editing,welcome,thank,user,time,articles,wikipedia,edit,page,talk\n",
      "Topic 15 [-4.870,-4.667,-4.735,-4.566,-5.183,-4.632]: people,thank,new,don,just,like,talk,wikipedia,page,article\n",
      "Topic 16 [-1.534,-1.382,-1.717,-1.267,-1.625,-1.309]: says,free,said,edits,thanks,personal,like,talk,good,article\n",
      "Topic 17 [3.838,-2.068,0.608,-2.626,0.471,-2.373]: ve,need,blocked,did,make,ll,say,way,think,just\n",
      "Topic 18 [4.438,0.950,4.930,-0.546,4.690,0.330]: wikipedia,did,right,just,know,people,don,like,fucking,fuck\n",
      "Topic 19 [-1.471,-1.267,-0.907,-1.223,-1.018,-0.914]: image,ll,add,reliable,list,actually,consider,let,source,article\n",
      "Topic 20 [1.374,-0.722,2.014,-0.386,1.279,0.080]: wear,proof,public,new,tried,unblock,general,types,unconstructive,fuck\n"
     ]
    }
   ],
   "source": [
    "cmf.print_topic_terms(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There now seems to be a lot more provocative and obscene topics. It's interesting to see how universal words like f\\*\\*k are in toxic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
